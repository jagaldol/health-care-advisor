{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from pinecone import Pinecone, Index\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSearchRetriever(BaseRetriever):\n",
    "\n",
    "    pinecone_index:Index\n",
    "    embedding_model:BGEM3FlagModel\n",
    "    alpha:float\n",
    "    top_k: int\n",
    "    min_score:float\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Sync implementations for retriever.\"\"\"\n",
    "        user_query_emb = self.embedding_model.encode(query, return_dense=True, return_sparse=True, return_colbert_vecs=False) #dense, sparse 둘 다 반환함\n",
    "        \n",
    "        query_dense_vector = user_query_emb['dense_vecs'].tolist()\n",
    "        user_query_sparse = user_query_emb['lexical_weights']\n",
    "        query_sparse_vector = {\n",
    "            'indices': [int(k) for k in user_query_sparse.keys() if k.isdigit()], #isdigit() 안하면 에러뜨더라\n",
    "            'values': [float(v) for k, v in user_query_sparse.items() if k.isdigit()]\n",
    "        }\n",
    "\n",
    "        hdense, hsparse = self._hybrid_score_norm(query_dense_vector, query_sparse_vector, alpha=self.alpha)\n",
    "\n",
    "        hybrid_query_response = self.pinecone_index.query(\n",
    "            top_k=self.top_k,\n",
    "            vector=hdense,\n",
    "            sparse_vector=hsparse,\n",
    "            include_metadata=True,\n",
    "        )\n",
    "        \n",
    "        documents = [\n",
    "            f\"{match['metadata']['answer_intro']}\\n\"\n",
    "            f\"{match['metadata']['answer_body']}\\n\"\n",
    "            f\"{match['metadata']['answer_conclusion']}\"\n",
    "            for match in hybrid_query_response['matches']\n",
    "            if match['score'] >= self.min_score\n",
    "        ]\n",
    "        return documents\n",
    "    \n",
    "    def _hybrid_score_norm(self, dense, sparse, alpha: float):\n",
    "        \"\"\"Hybrid score using a convex combination\n",
    "\n",
    "        alpha * dense + (1 - alpha) * sparse\n",
    "\n",
    "        Args:\n",
    "            dense: Array of floats representing\n",
    "            sparse: a dict of `indices` and `values`\n",
    "            alpha: scale between 0 and 1\n",
    "        \"\"\"\n",
    "        if alpha < 0 or alpha > 1:\n",
    "            raise ValueError(\"Alpha must be between 0 and 1\")\n",
    "        hs = {\n",
    "            'indices': sparse['indices'],\n",
    "            'values':  [v * (1 - alpha) for v in sparse['values']]\n",
    "        }\n",
    "        return [v * alpha for v in dense], hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5658697a839445f489e92123f63ba713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Connect to the existing Pinecone index\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "pinecone_index = pc.Index(\"health-care\")\n",
    "\n",
    "embedding_model = BGEM3FlagModel('BAAI/bge-m3',use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = HybridSearchRetriever(\n",
    "        pinecone_index=pinecone_index,\n",
    "        embedding_model=embedding_model,\n",
    "        alpha=0.95,\n",
    "        top_k=3,\n",
    "        min_score=0.55,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_score_norm(dense, sparse, alpha: float):\n",
    "    \"\"\"Hybrid score using a convex combination\n",
    "\n",
    "    alpha * dense + (1 - alpha) * sparse\n",
    "\n",
    "    Args:\n",
    "        dense: Array of floats representing\n",
    "        sparse: a dict of `indices` and `values`\n",
    "        alpha: scale between 0 and 1\n",
    "    \"\"\"\n",
    "    if alpha < 0 or alpha > 1:\n",
    "        raise ValueError(\"Alpha must be between 0 and 1\")\n",
    "    hs = {\n",
    "        'indices': sparse['indices'],\n",
    "        'values':  [v * (1 - alpha) for v in sparse['values']]\n",
    "    }\n",
    "    return [v * alpha for v in dense], hs\n",
    "\n",
    "def test(query):\n",
    "    user_query_emb = embedding_model.encode(query, return_dense=True, return_sparse=True, return_colbert_vecs=False) #dense, sparse 둘 다 반환함\n",
    "            \n",
    "    query_dense_vector = user_query_emb['dense_vecs'].tolist()\n",
    "    user_query_sparse = user_query_emb['lexical_weights']\n",
    "    query_sparse_vector = {\n",
    "        'indices': [int(k) for k in user_query_sparse.keys() if k.isdigit()], #isdigit() 안하면 에러뜨더라\n",
    "        'values': [float(v) for k, v in user_query_sparse.items() if k.isdigit()]\n",
    "    }\n",
    "\n",
    "    hdense, hsparse = hybrid_score_norm(query_dense_vector, query_sparse_vector, alpha=0.95)\n",
    "\n",
    "    hybrid_query_response = pinecone_index.query(\n",
    "        top_k=3,\n",
    "        vector=hdense,\n",
    "        sparse_vector=hsparse,\n",
    "        include_metadata=True,\n",
    "    )\n",
    "    print(hybrid_query_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': 'HC-A-02104685',\n",
      "              'metadata': {'answer_body': '이 질환의 초기 증상은 주로 추위나 스트레스 상황에서 손가락이나 '\n",
      "                                          '발가락이 창백해지고 감각이 둔화되는 것입니다. 이러한 증상은 '\n",
      "                                          '일시적으로 발생하며, 전신성 경화증이 동반된 경우 더욱 '\n",
      "                                          '심해집니다. 레이노현상은 따뜻한 환경에서 더욱 심해질 수 '\n",
      "                                          '있으므로 환우회나 의사를 방문하여 검사를 받는 것이 좋습니다.',\n",
      "                           'answer_conclusion': '손발이 차고 수족냉증이 있는 경우 레이노증후군 '\n",
      "                                                '가능성이 있으므로 의심 증상이 있다면 반드시 검사를 '\n",
      "                                                '받아 적절한 치료를 받아야 합니다.',\n",
      "                           'answer_intro': '레이노증후군은 혈관 반응 장애로 인해 손가락이나 발가락에서 '\n",
      "                                           '창백하거나 비정상적인 혈관의 수축 증상이 나타나는 질환입니다.',\n",
      "                           'department': '내과',\n",
      "                           'disease_category': '뇌신경정신질환',\n",
      "                           'disease_name_kor': '레이노병',\n",
      "                           'intention': '정의',\n",
      "                           'question': '레이노증후군은 무엇인가요?'},\n",
      "              'score': 0.420072615,\n",
      "              'values': []},\n",
      "             {'id': 'HC-A-02029184',\n",
      "              'metadata': {'answer_body': '레이노증후군은 주로 일시적인 증상으로 나타나며, 따뜻한 실내에 '\n",
      "                                          '있을 때 일시적으로 증상이 개선될 수 있습니다. 그러나 추운 '\n",
      "                                          '장소에서는 증상이 악화될 수 있습니다. 손발이 저리고 시리게 '\n",
      "                                          '되며, 혈관의 수축이 증가하는 것이 가장 일반적인 증상입니다. '\n",
      "                                          '증상은 따뜻한 환경에 있으면 개선되지만, 추운 장소에 있을 때 '\n",
      "                                          '손발을 따뜻하게 유지하는 것이 좋습니다.',\n",
      "                           'answer_conclusion': '레이노증후군은 주로 추위에 노출되거나 스트레스 상황에 '\n",
      "                                                '노출될 때 증상이 악화될 수 있습니다. 따뜻한 환경에 '\n",
      "                                                '있으면 증상이 완화될 수 있으며, 추운 장소에 있을 '\n",
      "                                                '때 손발을 따뜻하게 유지하는 것이 증상을 완화시키는 '\n",
      "                                                '데 도움이 됩니다.',\n",
      "                           'answer_intro': '레이노증후군은 손발이 차거나 저림과 같은 증상이 가장 일반적인 '\n",
      "                                           '증상으로 나타나는 질환입니다. 추위에 노출되거나 스트레스 '\n",
      "                                           '상황에 노출되면 증상이 더욱 심해질 수 있으며, 심각한 '\n",
      "                                           '경우에는 손발이 위축되어 환자 스스로는 잘 느끼지 못할 수 '\n",
      "                                           '있습니다.',\n",
      "                           'department': '내과',\n",
      "                           'disease_category': '뇌신경정신질환',\n",
      "                           'disease_name_kor': '레이노병',\n",
      "                           'intention': '정의',\n",
      "                           'question': '레이노증후군의 증상은 무엇인가요?'},\n",
      "              'score': 0.41374737,\n",
      "              'values': []},\n",
      "             {'id': 'HC-A-02193658',\n",
      "              'metadata': {'answer_body': '레이노현상은 손가락이나 발가락 등 손가락, 발가락, 발 부위의 '\n",
      "                                          '피부 및 혈관이 과도한 추위에 노출되거나 스트레스에 노출될 때 '\n",
      "                                          '발생할 수 있습니다. 이로 인해 피부가 하얗게 변하고, 혈관이 '\n",
      "                                          '수축하여 혈액의 공급이 제한됩니다. 추위로 인해 손이나 발가락 '\n",
      "                                          '등이 감각이 둔해지고 혈액이 원활히 흐르지 않게 되며, '\n",
      "                                          '손가락이나 발가락이 창백한 색으로 변하거나 통증을 유발할 수 '\n",
      "                                          '있습니다. 또한, 추위에 노출되거나 스트레스를 받는 상황에서는 '\n",
      "                                          '손가락이나 발가락 혈관이 경련과 염증을 일으켜 궤양이 생길 수도 '\n",
      "                                          '있습니다.',\n",
      "                           'answer_conclusion': '레이노증후군은 손가락, 발가락, 발 부위의 피부 및 '\n",
      "                                                '혈관에 대한 증상을 동반합니다. 추위에 노출되는 '\n",
      "                                                '상황에서는 손이나 발가락이 창백해지거나 통증이 발생할 '\n",
      "                                                '수 있으며, 손가락이나 발가락 혈관이 경련과 염증을 '\n",
      "                                                '일으킬 수도 있습니다. 따라서, 이러한 증상을 '\n",
      "                                                '경험한다면 의사와 상담하여 적절한 진단을 받아야 '\n",
      "                                                '합니다.',\n",
      "                           'answer_intro': '레이노증후군은 손가락이나 발가락의 혈관이 추위나 감정적인 '\n",
      "                                           '스트레스에 예민하여 혈류가 감소하는 상태입니다.',\n",
      "                           'department': '내과',\n",
      "                           'disease_category': '뇌신경정신질환',\n",
      "                           'disease_name_kor': '레이노병',\n",
      "                           'intention': '정의',\n",
      "                           'question': '레이노증후군이란 무엇이며, 어떤 상황에서 발생할 수 있나요?'},\n",
      "              'score': 0.405026436,\n",
      "              'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'read_units': 15}}\n"
     ]
    }
   ],
   "source": [
    "test(\"아데노\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b997211b106e400c8c7acb257ae75a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id=\"google/gemma-2-2b-it\"\n",
    "\n",
    "gemma_2_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "gemma_2_tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations  # type: ignore[import-not-found]\n",
    "\n",
    "from typing import Any, Iterator, List, Mapping, Optional\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import BaseLLM\n",
    "from langchain_core.outputs import Generation, GenerationChunk, LLMResult\n",
    "from pydantic import ConfigDict\n",
    "\n",
    "DEFAULT_MODEL_ID = \"gpt2\"\n",
    "DEFAULT_TASK = \"text-generation\"\n",
    "VALID_TASKS = (\n",
    "    \"text2text-generation\",\n",
    "    \"text-generation\",\n",
    "    \"summarization\",\n",
    "    \"translation\",\n",
    ")\n",
    "DEFAULT_BATCH_SIZE = 4\n",
    "\n",
    "\n",
    "class HuggingFacePipeline(BaseLLM):\n",
    "\n",
    "    pipeline: Any = None  #: :meta private:\n",
    "    model_id: str = DEFAULT_MODEL_ID\n",
    "    \"\"\"Model name to use.\"\"\"\n",
    "    model_kwargs: Optional[dict] = None\n",
    "    \"\"\"Keyword arguments passed to the model.\"\"\"\n",
    "    pipeline_kwargs: Optional[dict] = None\n",
    "    \"\"\"Keyword arguments passed to the pipeline.\"\"\"\n",
    "    batch_size: int = DEFAULT_BATCH_SIZE\n",
    "    \"\"\"Batch size to use when passing multiple documents to generate.\"\"\"\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        extra=\"forbid\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\n",
    "            \"model_id\": self.model_id,\n",
    "            \"model_kwargs\": self.model_kwargs,\n",
    "            \"pipeline_kwargs\": self.pipeline_kwargs,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"huggingface_pipeline\"\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> LLMResult:\n",
    "        # List to hold all results\n",
    "        text_generations: List[str] = []\n",
    "        pipeline_kwargs = kwargs.get(\"pipeline_kwargs\", {})\n",
    "        skip_prompt = kwargs.get(\"skip_prompt\", False)\n",
    "\n",
    "        for i in range(0, len(prompts), self.batch_size):\n",
    "            batch_prompts = prompts[i : i + self.batch_size]\n",
    "\n",
    "            # Process batch of prompts\n",
    "            responses = self.pipeline(\n",
    "                batch_prompts,\n",
    "                **pipeline_kwargs,\n",
    "            )\n",
    "\n",
    "            # Process each response in the batch\n",
    "            for j, response in enumerate(responses):\n",
    "                if isinstance(response, list):\n",
    "                    # if model returns multiple generations, pick the top one\n",
    "                    response = response[0]\n",
    "\n",
    "                if self.pipeline.task == \"text-generation\":\n",
    "                    text = response[\"generated_text\"]\n",
    "                elif self.pipeline.task == \"text2text-generation\":\n",
    "                    text = response[\"generated_text\"]\n",
    "                elif self.pipeline.task == \"summarization\":\n",
    "                    text = response[\"summary_text\"]\n",
    "                elif self.pipeline.task in \"translation\":\n",
    "                    text = response[\"translation_text\"]\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Got invalid task {self.pipeline.task}, \"\n",
    "                        f\"currently only {VALID_TASKS} are supported\"\n",
    "                    )\n",
    "                if skip_prompt:\n",
    "                    text = text[len(batch_prompts[j]) :]\n",
    "                # Append the processed text to results\n",
    "                text_generations.append(text)\n",
    "\n",
    "        return LLMResult(\n",
    "            generations=[[Generation(text=text)] for text in text_generations]\n",
    "        )\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[GenerationChunk]:\n",
    "        from threading import Thread\n",
    "\n",
    "        import torch\n",
    "        from transformers import (\n",
    "            StoppingCriteria,\n",
    "            StoppingCriteriaList,\n",
    "            TextIteratorStreamer,\n",
    "        )\n",
    "\n",
    "        pipeline_kwargs = kwargs.get(\"pipeline_kwargs\", {})\n",
    "        skip_prompt = kwargs.get(\"skip_prompt\", True)\n",
    "\n",
    "        if stop is not None:\n",
    "            stop = self.pipeline.tokenizer.convert_tokens_to_ids(stop)\n",
    "        stopping_ids_list = stop or []\n",
    "\n",
    "        class StopOnTokens(StoppingCriteria):\n",
    "            def __call__(\n",
    "                self,\n",
    "                input_ids: torch.LongTensor,\n",
    "                scores: torch.FloatTensor,\n",
    "                **kwargs: Any,\n",
    "            ) -> bool:\n",
    "                for stop_id in stopping_ids_list:\n",
    "                    if input_ids[0][-1] == stop_id:\n",
    "                        return True\n",
    "                return False\n",
    "\n",
    "        stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "        streamer = TextIteratorStreamer(\n",
    "            self.pipeline.tokenizer,\n",
    "            timeout=60.0,\n",
    "            skip_prompt=skip_prompt,\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        generation_kwargs = dict(\n",
    "            text_inputs= prompt,\n",
    "            streamer=streamer,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            **pipeline_kwargs,\n",
    "        )\n",
    "        t1 = Thread(target=self.pipeline, kwargs=generation_kwargs)\n",
    "        t1.start()\n",
    "\n",
    "        for char in streamer:\n",
    "            chunk = GenerationChunk(text=char)\n",
    "            if run_manager:\n",
    "                run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n",
    "\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_kwargs={\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": .5,\n",
    "    \"top_p\": 0.7,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "gen = pipeline(\n",
    "    task='text-generation',\n",
    "    model=gemma_2_model,\n",
    "    tokenizer=gemma_2_tokenizer,\n",
    "    device=device,\n",
    "    **pipeline_kwargs\n",
    "    )\n",
    "llm = HuggingFacePipeline(pipeline=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, template=\"<bos><start_of_turn>user\\nInstructions:\\n- If the question involves a health-related issue, suggest possible causes and basic steps the user can take for relief, if applicable.\\n- You should explain in as much detail as possible what you know from the bottom of your heart to the user's questions.\\n- You can refer to the contents of the documents to create a response.\\n- Only use information that is directly related to the question.\\n- If no information is found in the documents, provide an answer based on general knowledge without fabricating details.\\n- You MUST answer in Korean.\\n\\n\\nDocuments: {documents}\\n\\nQuestion: {question}<end_of_turn>\\n<start_of_turn>model\\n\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Instructions:\n",
    "- If the question involves a health-related issue, suggest possible causes and basic steps the user can take for relief, if applicable.\n",
    "- You should explain in as much detail as possible what you know from the bottom of your heart to the user's questions.\n",
    "- You can refer to the contents of the documents to create a response.\n",
    "- Only use information that is directly related to the question.\n",
    "- If no information is found in the documents, provide an answer based on general knowledge without fabricating details.\n",
    "- You MUST answer in Korean.\n",
    "\n",
    "\n",
    "Documents: {documents}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prev_chat = []\n",
    "\n",
    "chat = [\n",
    "    *prev_chat,\n",
    "     { \"role\": \"user\", \"content\": template}\n",
    "]\n",
    "\n",
    "prompt_template = gemma_2_tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"question\", \"documents\"], template=prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<bos><start_of_turn>user\\nInstructions:\\n- If the question involves a health-related issue, suggest possible causes and basic steps the user can take for relief, if applicable.\\n- You should explain in as much detail as possible what you know from the bottom of your heart to the user's questions.\\n- You can refer to the contents of the documents to create a response.\\n- Only use information that is directly related to the question.\\n- If no information is found in the documents, provide an answer based on general knowledge without fabricating details.\\n- You MUST answer in Korean.\\n\\n\\nDocuments: ['월경전 증후군은 월경 주기 전에 나타나는 다양한 신체적, 정신적 증상을 말합니다. 이는 여성들의 일상적인 생활에 영향을 주며, 월경전 불쾌한 증상으로 나타나기도 합니다.\\\\n월경전 증후군의 증상 중 하나는 배 아픔입니다. 일부 여성들은 배가 자주 아프고 불편함을 느낄 수 있습니다. 이러한 증상은 월경 전 몇 일에서 2주 전부터 시작되며 월경이 시작되면서 사라집니다. 복부 팽만감, 복통, 불안, 짜증, 우울 등이 월경전 증후군의 증상 중 일부일 수 있습니다.\\\\n월경전 증후군은 일시적으로 나타날 수도 있고 오랫동안 지속될 수도 있습니다. 월경전 증후군을 완화하기 위해 적절한 휴식과 충분한 수면을 취하는 것이 중요합니다. 일상 생활에서 신체적인 피로를 줄이고 정신적인 안정감을 유지하는 것도 도움이 됩니다. 그러나 증상이 지속되거나 심해지면 의사와 상담하여 적절한 치료 방법을 결정해야 합니다.', '배뇨 장애는 다양한 증상을 보일 수 있습니다.\\\\n빈뇨, 야간뇨, 요주저, 세뇨, 단절성배뇨, 요점적, 복부 힘주기, 요절박이 주요한 증상입니다. 빈뇨는 하루 평균 배뇨 횟수가 8회 이상이며 야간뇨는 잠을 자다가 화장실을 2회 이상 가야 할 정도로 자주 소변을 보는 증상입니다. 요주저는 소변줄기가 약해지고 약해지는 증상입니다. 세뇨는 소변줄기가 약해지고 약해지며, 요점적은 배뇨 후 소변이 방울방울로 떨어지는 증상입니다. 복부 힘주기는 배뇨를 하기 위해 복부에 힘을 줘야 하는 증상입니다. 마지막으로, 요절박은 소변을 참을 수 없는 경우에 소변이 나오는 것을 말합니다.\\\\n배뇨 장애는 다양한 증상을 동반할 수 있으며, 이는 비뇨기, 신장, 신경계, 근골격계의 문제로 인해 발생할 수 있습니다. 따라서 증상이 있을 경우 전문의의 진료를 받아 원인을 확인하고 치료받는 것이 중요합니다.', '배뇨장애는 배뇨가 원활하지 않는 증상을 말합니다. 이러한 증상들은 다양한 원인에 의해 발생할 수 있습니다.\\\\n일반적으로 배뇨장애의 증상으로는 밤사이에 자주 깨는 빈뇨, 야간뇨, 요주저, 배뇨 시 통증, 잔뇨감, 소변줄기가 약해지는 세뇨, 소변이 중간중간 끊어지는 단절성배뇨, 소변이 나오기까지 시간이 걸리는 요점적 등이 있습니다. 이러한 증상들은 방광이 충분히 수축하지 못하고 비정상적으로 작은 압력으로 인해 발생할 수 있습니다. 배뇨장애는 일반적으로 통증, 혈뇨, 탁한 소변, 빈뇨 등의 증상을 동반할 수 있습니다.\\\\n배뇨장애는 다양한 증상이 있을 수 있으며, 증상의 심각성과 원인은 개인에 따라 다를 수 있습니다. 만약 배뇨장애 증상을 경험한다면 전문의와 상담하여 적절한 치료방법을 찾아야 합니다.']\\n\\nQuestion: 갑자기 배가 너무 아파<end_of_turn>\\n<start_of_turn>model\\n월경전 증후군의 증상 중 하나로 배가 아픈 것은 매우 흔한 증상입니다. 월경전 몇 일에서 2주 전부터 시작되며 월경이 시작되면서 사라집니다. \\n\\n**월경전 증후군의 배 아픔은 다음과 같은 원인으로 발생할 수 있습니다.**\\n\\n* **호르몬 변화:** 월경 전에는 호르몬 변화가 일어나며, 이는 배 내부의 조직에도 영향을 미쳐 배 아픔을 유발할 수 있습니다.\\n* **스트레스:** 월경 전 스트레스는 호르몬 분비를 변화시키고, 이로 인해 배 아픔을 유발할 수 있습니다.\\n\\n**월경전 증후군의 배 아픔을 완화하기 위한 방법은 다음과 같습니다.**\\n\\n* **충분한 수면:** 충분한 수면은 스트레스 해소 및 호르몬 조절에 도움이 될 수 있습니다.\\n* **휴식:** 일상생활에서 신체적인 피로를 줄이는 것은 월경전 증후군의 증상 완화에 도움이 됩니다.\\n* **스트레스 관리:** 스트레스를 줄이는 것은 월경전 증후군의 악화를 예방하는 데 효과적입니다.\\n\\n**하지만, 배 아픔이 지속되거나 심각해질 경우, 의사에게 문의하여 적절한 치료를 받으십시오.** \\n\\n\\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = rag_chain.invoke(\"갑자기 배가 너무 아파\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "월경전 증후군의 증상 중 하나로 배 아픔이 나타날 수 있습니다. 월경 전 몇 일에서 2주 전부터 시작되며 월경이 시작되면서 사라집니다. 갑작스러운 배 아픔은 월경전 증후군의 증상일 수 있지만, 다른 질환의 증상일 수도 있습니다.  \n",
      "\n",
      "**월경전 증후군의 경우:**\n",
      "\n",
      "* **휴식과 충분한 수면**: 월경전 증후군의 증상을 완화하기 위해 적절한 휴식과 충분한 수면을 취하는 것이 중요합니다. \n",
      "* **일상생활에서 신체적인 피로를 줄이고 정신적인 안정감을 유지하는 것도 도움이 됩니다.**\n",
      "\n",
      "**하지만, 갑작스러운 배 아픔은 다른 질환의 증상일 수 있으므로 다음과 같은 방법들을 고려해 볼 수 있습니다.**\n",
      "\n",
      "1. **의사와 상담**: 갑작스러운 배 아픔이 지속되거나 심각하거나 특별한 증상이 있는 경우, 의사에게 문의하여 진단을 받아보세요. \n",
      "2. **복용 가능한 약물**: 의사의 처방을 받아 복용하는 약물이 필요할 수 있습니다.\n",
      "\n",
      "\n",
      "**참고:** 위 내용은 일반적인 정보 제공 목적이며 의학적 조언이 아닙니다. 궁금한 점이 있다면 의사와 상담하세요. \n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.stream(\"갑자기 배가 너무 아파\")\n",
    "for chunk in answer:\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
