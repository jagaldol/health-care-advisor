{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 09:01:06.690805: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-14 09:01:06.690957: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-14 09:01:06.691015: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-14 09:01:06.702490: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-14 09:01:07.540401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from pinecone import Pinecone, Index\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSearchRetriever(BaseRetriever):\n",
    "\n",
    "    k: int\n",
    "    \"\"\"Number of top results to return\"\"\"\n",
    "    embedding_model:BGEM3FlagModel\n",
    "    \n",
    "    pinecone_index:Index\n",
    "\n",
    "    alpha:float\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Sync implementations for retriever.\"\"\"\n",
    "        user_query_emb = self.embedding_model.encode(query, return_dense=True, return_sparse=True, return_colbert_vecs=False) #dense, sparse 둘 다 반환함\n",
    "        \n",
    "        query_dense_vector = user_query_emb['dense_vecs'].tolist()\n",
    "        user_query_sparse = user_query_emb['lexical_weights']\n",
    "        query_sparse_vector = {\n",
    "            'indices': [int(k) for k in user_query_sparse.keys() if k.isdigit()], #isdigit() 안하면 에러뜨더라\n",
    "            'values': [float(v) for k, v in user_query_sparse.items() if k.isdigit()]\n",
    "        }\n",
    "\n",
    "        hdense, hsparse = self._hybrid_score_norm(query_dense_vector, query_sparse_vector, alpha=self.alpha)\n",
    "\n",
    "        hybrid_query_response = self.pinecone_index.query(\n",
    "            # namespace=\"example-namespace\",\n",
    "            top_k=3,\n",
    "            vector=hdense,\n",
    "            sparse_vector=hsparse,\n",
    "            include_metadata=True,\n",
    "        )\n",
    "        \n",
    "        documents = [\n",
    "            f\"{match['metadata']['answer_intro']}\\n\"\n",
    "            f\"{match['metadata']['answer_body']}\\n\"\n",
    "            f\"{match['metadata']['answer_conclusion']}\"\n",
    "            for match in hybrid_query_response['matches']\n",
    "        ]\n",
    "        return documents\n",
    "    \n",
    "    def _hybrid_score_norm(self, dense, sparse, alpha: float):\n",
    "        \"\"\"Hybrid score using a convex combination\n",
    "\n",
    "        alpha * dense + (1 - alpha) * sparse\n",
    "\n",
    "        Args:\n",
    "            dense: Array of floats representing\n",
    "            sparse: a dict of `indices` and `values`\n",
    "            alpha: scale between 0 and 1\n",
    "        \"\"\"\n",
    "        if alpha < 0 or alpha > 1:\n",
    "            raise ValueError(\"Alpha must be between 0 and 1\")\n",
    "        hs = {\n",
    "            'indices': sparse['indices'],\n",
    "            'values':  [v * (1 - alpha) for v in sparse['values']]\n",
    "        }\n",
    "        return [v * alpha for v in dense], hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_model(model_name):\n",
    "    model_path = os.path.join('models', model_name.replace('/', '_'))  # 'models/BAAI_bge-m3'\n",
    "    # 모델이 로컬에 존재하는지 확인\n",
    "    if not os.path.exists(model_path):\n",
    "        # 모델과 토크나이저 다운로드 및 저장\n",
    "        print(f\"모델을 다운로드 중입니다: {model_name}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # 모델과 토크나이저 저장\n",
    "        tokenizer.save_pretrained(model_path)\n",
    "        model.save_pretrained(model_path)\n",
    "        print(f\"모델이 {model_path}에 저장되었습니다.\")\n",
    "    else:\n",
    "        # 모델과 토크나이저 로드\n",
    "        print(f\"모델을 {model_path}에서 로드 중입니다.\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModel.from_pretrained(model_path)\n",
    "        print(f\"모델이 로드되었습니다.\")\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델을 models/BAAI_bge-m3에서 로드 중입니다.\n",
      "모델이 로드되었습니다.\n"
     ]
    }
   ],
   "source": [
    "m3_tokenizer, m3_model = load_model('BAAI/bge-m3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'PINECONE_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pc \u001b[38;5;241m=\u001b[39m Pinecone(api_key\u001b[38;5;241m=\u001b[39m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPINECONE_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Connect to the existing Pinecone index\u001b[39;00m\n\u001b[1;32m      3\u001b[0m pinecone_index \u001b[38;5;241m=\u001b[39m pc\u001b[38;5;241m.\u001b[39mIndex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhealth-care\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:680\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'PINECONE_API_KEY'"
     ]
    }
   ],
   "source": [
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "# Connect to the existing Pinecone index\n",
    "pinecone_index = pc.Index(\"health-care\")\n",
    "\n",
    "embedding_model = BGEM3FlagModel(m3_model,use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "retriever = HybridSearchRetriever(k=3, embedding_model=embedding_model, pinecone_index=pinecone_index, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_2_tokenizer, gemma_2_model = load_model('google/gemma-2-2b-it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = pipeline(\n",
    "    task='text-generation',\n",
    "    model=gemma_2_model,\n",
    "    tokenizer=gemma_2_tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    )\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. 당신의 임무는 주어진 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.\n",
    "검색된 다음 문맥(context) 을 사용하여 질문(question) 에 답하세요. 만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.\n",
    "한글로 답변해 주세요. 단, 기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요.\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Context:\n",
    "{context}\n",
    "\n",
    "#Answer:\"\"\"\n",
    "\n",
    "prev_chat = []\n",
    "\n",
    "chat = [\n",
    "    *prev_chat,\n",
    "     { \"role\": \"user\", \"content\": template}\n",
    "]\n",
    "\n",
    "prompt_template = gemma_2_tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"question\", \"context\"], template=prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_response(response, return_output=False):\n",
    "    answer = \"\"\n",
    "    for token in response:\n",
    "        answer += token\n",
    "        print(token, end=\"\", flush=True)\n",
    "    if return_output:\n",
    "        return answer\n",
    "answer = rag_chain.invoke(\"갑자기 너무 배가 아파\")\n",
    "stream_response(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
