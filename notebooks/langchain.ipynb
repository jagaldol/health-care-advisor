{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 14:06:27.104924: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-16 14:06:27.104988: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-16 14:06:27.105037: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-16 14:06:27.114370: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-16 14:06:27.907429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from pinecone import Pinecone, Index\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSearchRetriever(BaseRetriever):\n",
    "\n",
    "    pinecone_index:Index\n",
    "    embedding_model:BGEM3FlagModel\n",
    "    alpha:float\n",
    "    top_k: int\n",
    "    min_score:float\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Sync implementations for retriever.\"\"\"\n",
    "        user_query_emb = self.embedding_model.encode(query, return_dense=True, return_sparse=True, return_colbert_vecs=False) #dense, sparse 둘 다 반환함\n",
    "        \n",
    "        query_dense_vector = user_query_emb['dense_vecs'].tolist()\n",
    "        user_query_sparse = user_query_emb['lexical_weights']\n",
    "        query_sparse_vector = {\n",
    "            'indices': [int(k) for k in user_query_sparse.keys() if k.isdigit()], #isdigit() 안하면 에러뜨더라\n",
    "            'values': [float(v) for k, v in user_query_sparse.items() if k.isdigit()]\n",
    "        }\n",
    "\n",
    "        hdense, hsparse = self._hybrid_score_norm(query_dense_vector, query_sparse_vector, alpha=self.alpha)\n",
    "\n",
    "        hybrid_query_response = self.pinecone_index.query(\n",
    "            top_k=self.top_k,\n",
    "            vector=hdense,\n",
    "            sparse_vector=hsparse,\n",
    "            include_metadata=True,\n",
    "        )\n",
    "        \n",
    "        documents = [\n",
    "            f\"{match['metadata']['answer_intro']}\\n\"\n",
    "            f\"{match['metadata']['answer_body']}\\n\"\n",
    "            f\"{match['metadata']['answer_conclusion']}\"\n",
    "            for match in hybrid_query_response['matches']\n",
    "            if match['score'] >= self.min_score\n",
    "        ]\n",
    "        return documents\n",
    "    \n",
    "    def _hybrid_score_norm(self, dense, sparse, alpha: float):\n",
    "        \"\"\"Hybrid score using a convex combination\n",
    "\n",
    "        alpha * dense + (1 - alpha) * sparse\n",
    "\n",
    "        Args:\n",
    "            dense: Array of floats representing\n",
    "            sparse: a dict of `indices` and `values`\n",
    "            alpha: scale between 0 and 1\n",
    "        \"\"\"\n",
    "        if alpha < 0 or alpha > 1:\n",
    "            raise ValueError(\"Alpha must be between 0 and 1\")\n",
    "        hs = {\n",
    "            'indices': sparse['indices'],\n",
    "            'values':  [v * (1 - alpha) for v in sparse['values']]\n",
    "        }\n",
    "        return [v * alpha for v in dense], hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9497807574e46c1827f7c0cf7339a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/FlagEmbedding/BGE_M3/modeling.py:335: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  colbert_state_dict = torch.load(os.path.join(model_dir, 'colbert_linear.pt'), map_location='cpu')\n",
      "/usr/local/lib/python3.10/dist-packages/FlagEmbedding/BGE_M3/modeling.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sparse_state_dict = torch.load(os.path.join(model_dir, 'sparse_linear.pt'), map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Connect to the existing Pinecone index\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "pinecone_index = pc.Index(\"health-care\")\n",
    "\n",
    "embedding_model = BGEM3FlagModel('BAAI/bge-m3',use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = HybridSearchRetriever(\n",
    "        pinecone_index=pinecone_index,\n",
    "        embedding_model=embedding_model,\n",
    "        alpha=0.95,\n",
    "        top_k=3,\n",
    "        min_score=0.55,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_score_norm(dense, sparse, alpha: float):\n",
    "    \"\"\"Hybrid score using a convex combination\n",
    "\n",
    "    alpha * dense + (1 - alpha) * sparse\n",
    "\n",
    "    Args:\n",
    "        dense: Array of floats representing\n",
    "        sparse: a dict of `indices` and `values`\n",
    "        alpha: scale between 0 and 1\n",
    "    \"\"\"\n",
    "    if alpha < 0 or alpha > 1:\n",
    "        raise ValueError(\"Alpha must be between 0 and 1\")\n",
    "    hs = {\n",
    "        'indices': sparse['indices'],\n",
    "        'values':  [v * (1 - alpha) for v in sparse['values']]\n",
    "    }\n",
    "    return [v * alpha for v in dense], hs\n",
    "\n",
    "def test(query):\n",
    "    user_query_emb = embedding_model.encode(query, return_dense=True, return_sparse=True, return_colbert_vecs=False) #dense, sparse 둘 다 반환함\n",
    "            \n",
    "    query_dense_vector = user_query_emb['dense_vecs'].tolist()\n",
    "    user_query_sparse = user_query_emb['lexical_weights']\n",
    "    query_sparse_vector = {\n",
    "        'indices': [int(k) for k in user_query_sparse.keys() if k.isdigit()], #isdigit() 안하면 에러뜨더라\n",
    "        'values': [float(v) for k, v in user_query_sparse.items() if k.isdigit()]\n",
    "    }\n",
    "\n",
    "    hdense, hsparse = hybrid_score_norm(query_dense_vector, query_sparse_vector, alpha=0.95)\n",
    "\n",
    "    hybrid_query_response = pinecone_index.query(\n",
    "        top_k=3,\n",
    "        vector=hdense,\n",
    "        sparse_vector=hsparse,\n",
    "        include_metadata=True,\n",
    "    )\n",
    "    print(hybrid_query_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': 'HC-A-02104685',\n",
      "              'metadata': {'answer_body': '이 질환의 초기 증상은 주로 추위나 스트레스 상황에서 손가락이나 '\n",
      "                                          '발가락이 창백해지고 감각이 둔화되는 것입니다. 이러한 증상은 '\n",
      "                                          '일시적으로 발생하며, 전신성 경화증이 동반된 경우 더욱 '\n",
      "                                          '심해집니다. 레이노현상은 따뜻한 환경에서 더욱 심해질 수 '\n",
      "                                          '있으므로 환우회나 의사를 방문하여 검사를 받는 것이 좋습니다.',\n",
      "                           'answer_conclusion': '손발이 차고 수족냉증이 있는 경우 레이노증후군 '\n",
      "                                                '가능성이 있으므로 의심 증상이 있다면 반드시 검사를 '\n",
      "                                                '받아 적절한 치료를 받아야 합니다.',\n",
      "                           'answer_intro': '레이노증후군은 혈관 반응 장애로 인해 손가락이나 발가락에서 '\n",
      "                                           '창백하거나 비정상적인 혈관의 수축 증상이 나타나는 질환입니다.',\n",
      "                           'department': '내과',\n",
      "                           'disease_category': '뇌신경정신질환',\n",
      "                           'disease_name_kor': '레이노병',\n",
      "                           'intention': '정의',\n",
      "                           'question': '레이노증후군은 무엇인가요?'},\n",
      "              'score': 0.420072615,\n",
      "              'values': []},\n",
      "             {'id': 'HC-A-02029184',\n",
      "              'metadata': {'answer_body': '레이노증후군은 주로 일시적인 증상으로 나타나며, 따뜻한 실내에 '\n",
      "                                          '있을 때 일시적으로 증상이 개선될 수 있습니다. 그러나 추운 '\n",
      "                                          '장소에서는 증상이 악화될 수 있습니다. 손발이 저리고 시리게 '\n",
      "                                          '되며, 혈관의 수축이 증가하는 것이 가장 일반적인 증상입니다. '\n",
      "                                          '증상은 따뜻한 환경에 있으면 개선되지만, 추운 장소에 있을 때 '\n",
      "                                          '손발을 따뜻하게 유지하는 것이 좋습니다.',\n",
      "                           'answer_conclusion': '레이노증후군은 주로 추위에 노출되거나 스트레스 상황에 '\n",
      "                                                '노출될 때 증상이 악화될 수 있습니다. 따뜻한 환경에 '\n",
      "                                                '있으면 증상이 완화될 수 있으며, 추운 장소에 있을 '\n",
      "                                                '때 손발을 따뜻하게 유지하는 것이 증상을 완화시키는 '\n",
      "                                                '데 도움이 됩니다.',\n",
      "                           'answer_intro': '레이노증후군은 손발이 차거나 저림과 같은 증상이 가장 일반적인 '\n",
      "                                           '증상으로 나타나는 질환입니다. 추위에 노출되거나 스트레스 '\n",
      "                                           '상황에 노출되면 증상이 더욱 심해질 수 있으며, 심각한 '\n",
      "                                           '경우에는 손발이 위축되어 환자 스스로는 잘 느끼지 못할 수 '\n",
      "                                           '있습니다.',\n",
      "                           'department': '내과',\n",
      "                           'disease_category': '뇌신경정신질환',\n",
      "                           'disease_name_kor': '레이노병',\n",
      "                           'intention': '정의',\n",
      "                           'question': '레이노증후군의 증상은 무엇인가요?'},\n",
      "              'score': 0.41374737,\n",
      "              'values': []},\n",
      "             {'id': 'HC-A-02193658',\n",
      "              'metadata': {'answer_body': '레이노현상은 손가락이나 발가락 등 손가락, 발가락, 발 부위의 '\n",
      "                                          '피부 및 혈관이 과도한 추위에 노출되거나 스트레스에 노출될 때 '\n",
      "                                          '발생할 수 있습니다. 이로 인해 피부가 하얗게 변하고, 혈관이 '\n",
      "                                          '수축하여 혈액의 공급이 제한됩니다. 추위로 인해 손이나 발가락 '\n",
      "                                          '등이 감각이 둔해지고 혈액이 원활히 흐르지 않게 되며, '\n",
      "                                          '손가락이나 발가락이 창백한 색으로 변하거나 통증을 유발할 수 '\n",
      "                                          '있습니다. 또한, 추위에 노출되거나 스트레스를 받는 상황에서는 '\n",
      "                                          '손가락이나 발가락 혈관이 경련과 염증을 일으켜 궤양이 생길 수도 '\n",
      "                                          '있습니다.',\n",
      "                           'answer_conclusion': '레이노증후군은 손가락, 발가락, 발 부위의 피부 및 '\n",
      "                                                '혈관에 대한 증상을 동반합니다. 추위에 노출되는 '\n",
      "                                                '상황에서는 손이나 발가락이 창백해지거나 통증이 발생할 '\n",
      "                                                '수 있으며, 손가락이나 발가락 혈관이 경련과 염증을 '\n",
      "                                                '일으킬 수도 있습니다. 따라서, 이러한 증상을 '\n",
      "                                                '경험한다면 의사와 상담하여 적절한 진단을 받아야 '\n",
      "                                                '합니다.',\n",
      "                           'answer_intro': '레이노증후군은 손가락이나 발가락의 혈관이 추위나 감정적인 '\n",
      "                                           '스트레스에 예민하여 혈류가 감소하는 상태입니다.',\n",
      "                           'department': '내과',\n",
      "                           'disease_category': '뇌신경정신질환',\n",
      "                           'disease_name_kor': '레이노병',\n",
      "                           'intention': '정의',\n",
      "                           'question': '레이노증후군이란 무엇이며, 어떤 상황에서 발생할 수 있나요?'},\n",
      "              'score': 0.405026436,\n",
      "              'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'read_units': 15}}\n"
     ]
    }
   ],
   "source": [
    "test(\"아데노\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae178e116cf485ba81982fda7bf22e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id=\"google/gemma-2-2b-it\"\n",
    "\n",
    "gemma_2_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "gemma_2_tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations  # type: ignore[import-not-found]\n",
    "\n",
    "import importlib.util\n",
    "import logging\n",
    "from typing import Any, Iterator, List, Mapping, Optional\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import BaseLLM\n",
    "from langchain_core.outputs import Generation, GenerationChunk, LLMResult\n",
    "from pydantic import ConfigDict\n",
    "\n",
    "DEFAULT_MODEL_ID = \"gpt2\"\n",
    "DEFAULT_TASK = \"text-generation\"\n",
    "VALID_TASKS = (\n",
    "    \"text2text-generation\",\n",
    "    \"text-generation\",\n",
    "    \"summarization\",\n",
    "    \"translation\",\n",
    ")\n",
    "DEFAULT_BATCH_SIZE = 4\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class HuggingFacePipeline(BaseLLM):\n",
    "    \"\"\"HuggingFace Pipeline API.\n",
    "\n",
    "    To use, you should have the ``transformers`` python package installed.\n",
    "\n",
    "    Only supports `text-generation`, `text2text-generation`, `summarization` and\n",
    "    `translation`  for now.\n",
    "\n",
    "    Example using from_model_id:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain_huggingface import HuggingFacePipeline\n",
    "            hf = HuggingFacePipeline.from_model_id(\n",
    "                model_id=\"gpt2\",\n",
    "                task=\"text-generation\",\n",
    "                pipeline_kwargs={\"max_new_tokens\": 10},\n",
    "            )\n",
    "    Example passing pipeline in directly:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain_huggingface import HuggingFacePipeline\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "            model_id = \"gpt2\"\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n",
    "            )\n",
    "            hf = HuggingFacePipeline(pipeline=pipe)\n",
    "    \"\"\"\n",
    "\n",
    "    pipeline: Any = None  #: :meta private:\n",
    "    model_id: str = DEFAULT_MODEL_ID\n",
    "    \"\"\"Model name to use.\"\"\"\n",
    "    model_kwargs: Optional[dict] = None\n",
    "    \"\"\"Keyword arguments passed to the model.\"\"\"\n",
    "    pipeline_kwargs: Optional[dict] = None\n",
    "    \"\"\"Keyword arguments passed to the pipeline.\"\"\"\n",
    "    batch_size: int = DEFAULT_BATCH_SIZE\n",
    "    \"\"\"Batch size to use when passing multiple documents to generate.\"\"\"\n",
    "    device: Optional[int] = -1\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        extra=\"forbid\",\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def from_model_id(\n",
    "        cls,\n",
    "        model_id: str,\n",
    "        task: str,\n",
    "        backend: str = \"default\",\n",
    "        device: Optional[int] = -1,\n",
    "        device_map: Optional[str] = None,\n",
    "        model_kwargs: Optional[dict] = None,\n",
    "        pipeline_kwargs: Optional[dict] = None,\n",
    "        batch_size: int = DEFAULT_BATCH_SIZE,\n",
    "        **kwargs: Any,\n",
    "    ) -> HuggingFacePipeline:\n",
    "        \"\"\"Construct the pipeline object from model_id and task.\"\"\"\n",
    "        try:\n",
    "            from transformers import (  # type: ignore[import]\n",
    "                AutoModelForCausalLM,\n",
    "                AutoModelForSeq2SeqLM,\n",
    "                AutoTokenizer,\n",
    "            )\n",
    "            from transformers import pipeline as hf_pipeline  # type: ignore[import]\n",
    "\n",
    "        except ImportError:\n",
    "            raise ValueError(\n",
    "                \"Could not import transformers python package. \"\n",
    "                \"Please install it with `pip install transformers`.\"\n",
    "            )\n",
    "\n",
    "        _model_kwargs = model_kwargs or {}\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, **_model_kwargs)\n",
    "\n",
    "        try:\n",
    "            if task == \"text-generation\":\n",
    "                if backend == \"openvino\":\n",
    "                    try:\n",
    "                        from optimum.intel.openvino import (  # type: ignore[import]\n",
    "                            OVModelForCausalLM,\n",
    "                        )\n",
    "\n",
    "                    except ImportError:\n",
    "                        raise ValueError(\n",
    "                            \"Could not import optimum-intel python package. \"\n",
    "                            \"Please install it with: \"\n",
    "                            \"pip install 'optimum[openvino,nncf]' \"\n",
    "                        )\n",
    "                    try:\n",
    "                        # use local model\n",
    "                        model = OVModelForCausalLM.from_pretrained(\n",
    "                            model_id, **_model_kwargs\n",
    "                        )\n",
    "\n",
    "                    except Exception:\n",
    "                        # use remote model\n",
    "                        model = OVModelForCausalLM.from_pretrained(\n",
    "                            model_id, export=True, **_model_kwargs\n",
    "                        )\n",
    "                else:\n",
    "                    model = AutoModelForCausalLM.from_pretrained(\n",
    "                        model_id, **_model_kwargs\n",
    "                    )\n",
    "            elif task in (\"text2text-generation\", \"summarization\", \"translation\"):\n",
    "                if backend == \"openvino\":\n",
    "                    try:\n",
    "                        from optimum.intel.openvino import OVModelForSeq2SeqLM\n",
    "\n",
    "                    except ImportError:\n",
    "                        raise ValueError(\n",
    "                            \"Could not import optimum-intel python package. \"\n",
    "                            \"Please install it with: \"\n",
    "                            \"pip install 'optimum[openvino,nncf]' \"\n",
    "                        )\n",
    "                    try:\n",
    "                        # use local model\n",
    "                        model = OVModelForSeq2SeqLM.from_pretrained(\n",
    "                            model_id, **_model_kwargs\n",
    "                        )\n",
    "\n",
    "                    except Exception:\n",
    "                        # use remote model\n",
    "                        model = OVModelForSeq2SeqLM.from_pretrained(\n",
    "                            model_id, export=True, **_model_kwargs\n",
    "                        )\n",
    "                else:\n",
    "                    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                        model_id, **_model_kwargs\n",
    "                    )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Got invalid task {task}, \"\n",
    "                    f\"currently only {VALID_TASKS} are supported\"\n",
    "                )\n",
    "        except ImportError as e:\n",
    "            raise ValueError(\n",
    "                f\"Could not load the {task} model due to missing dependencies.\"\n",
    "            ) from e\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "        if (\n",
    "            (\n",
    "                getattr(model, \"is_loaded_in_4bit\", False)\n",
    "                or getattr(model, \"is_loaded_in_8bit\", False)\n",
    "            )\n",
    "            and device is not None\n",
    "            and backend == \"default\"\n",
    "        ):\n",
    "            logger.warning(\n",
    "                f\"Setting the `device` argument to None from {device} to avoid \"\n",
    "                \"the error caused by attempting to move the model that was already \"\n",
    "                \"loaded on the GPU using the Accelerate module to the same or \"\n",
    "                \"another device.\"\n",
    "            )\n",
    "            device = None\n",
    "\n",
    "        if (\n",
    "            device is not None\n",
    "            and importlib.util.find_spec(\"torch\") is not None\n",
    "            and backend == \"default\"\n",
    "        ):\n",
    "            import torch\n",
    "\n",
    "            cuda_device_count = torch.cuda.device_count()\n",
    "            if device < -1 or (device >= cuda_device_count):\n",
    "                raise ValueError(\n",
    "                    f\"Got device=={device}, \"\n",
    "                    f\"device is required to be within [-1, {cuda_device_count})\"\n",
    "                )\n",
    "            if device_map is not None and device < 0:\n",
    "                device = None\n",
    "            if device is not None and device < 0 and cuda_device_count > 0:\n",
    "                logger.warning(\n",
    "                    \"Device has %d GPUs available. \"\n",
    "                    \"Provide device={deviceId} to `from_model_id` to use available\"\n",
    "                    \"GPUs for execution. deviceId is -1 (default) for CPU and \"\n",
    "                    \"can be a positive integer associated with CUDA device id.\",\n",
    "                    cuda_device_count,\n",
    "                )\n",
    "        if device is not None and device_map is not None and backend == \"openvino\":\n",
    "            logger.warning(\"Please set device for OpenVINO through: `model_kwargs`\")\n",
    "        if \"trust_remote_code\" in _model_kwargs:\n",
    "            _model_kwargs = {\n",
    "                k: v for k, v in _model_kwargs.items() if k != \"trust_remote_code\"\n",
    "            }\n",
    "        _pipeline_kwargs = pipeline_kwargs or {}\n",
    "        pipeline = hf_pipeline(\n",
    "            task=task,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device,\n",
    "            device_map=device_map,\n",
    "            batch_size=batch_size,\n",
    "            model_kwargs=_model_kwargs,\n",
    "            **_pipeline_kwargs,\n",
    "        )\n",
    "        if pipeline.task not in VALID_TASKS:\n",
    "            raise ValueError(\n",
    "                f\"Got invalid task {pipeline.task}, \"\n",
    "                f\"currently only {VALID_TASKS} are supported\"\n",
    "            )\n",
    "        return cls(\n",
    "            pipeline=pipeline,\n",
    "            model_id=model_id,\n",
    "            model_kwargs=_model_kwargs,\n",
    "            pipeline_kwargs=_pipeline_kwargs,\n",
    "            batch_size=batch_size,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\n",
    "            \"model_id\": self.model_id,\n",
    "            \"model_kwargs\": self.model_kwargs,\n",
    "            \"pipeline_kwargs\": self.pipeline_kwargs,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"huggingface_pipeline\"\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> LLMResult:\n",
    "        # List to hold all results\n",
    "        text_generations: List[str] = []\n",
    "        pipeline_kwargs = kwargs.get(\"pipeline_kwargs\", {})\n",
    "        skip_prompt = kwargs.get(\"skip_prompt\", False)\n",
    "\n",
    "        for i in range(0, len(prompts), self.batch_size):\n",
    "            batch_prompts = prompts[i : i + self.batch_size]\n",
    "\n",
    "            # Process batch of prompts\n",
    "            responses = self.pipeline(\n",
    "                batch_prompts,\n",
    "                **pipeline_kwargs,\n",
    "            )\n",
    "\n",
    "            # Process each response in the batch\n",
    "            for j, response in enumerate(responses):\n",
    "                if isinstance(response, list):\n",
    "                    # if model returns multiple generations, pick the top one\n",
    "                    response = response[0]\n",
    "\n",
    "                if self.pipeline.task == \"text-generation\":\n",
    "                    text = response[\"generated_text\"]\n",
    "                elif self.pipeline.task == \"text2text-generation\":\n",
    "                    text = response[\"generated_text\"]\n",
    "                elif self.pipeline.task == \"summarization\":\n",
    "                    text = response[\"summary_text\"]\n",
    "                elif self.pipeline.task in \"translation\":\n",
    "                    text = response[\"translation_text\"]\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Got invalid task {self.pipeline.task}, \"\n",
    "                        f\"currently only {VALID_TASKS} are supported\"\n",
    "                    )\n",
    "                if skip_prompt:\n",
    "                    text = text[len(batch_prompts[j]) :]\n",
    "                # Append the processed text to results\n",
    "                text_generations.append(text)\n",
    "\n",
    "        return LLMResult(\n",
    "            generations=[[Generation(text=text)] for text in text_generations]\n",
    "        )\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[GenerationChunk]:\n",
    "        from threading import Thread\n",
    "\n",
    "        import torch\n",
    "        from transformers import (\n",
    "            StoppingCriteria,\n",
    "            StoppingCriteriaList,\n",
    "            TextIteratorStreamer,\n",
    "        )\n",
    "\n",
    "        pipeline_kwargs = self.pipeline_kwargs if self.pipeline_kwargs != None else {}\n",
    "        skip_prompt = kwargs.get(\"skip_prompt\", True)\n",
    "\n",
    "        if stop is not None:\n",
    "            stop = self.pipeline.tokenizer.convert_tokens_to_ids(stop)\n",
    "        stopping_ids_list = stop or []\n",
    "\n",
    "        class StopOnTokens(StoppingCriteria):\n",
    "            def __call__(\n",
    "                self,\n",
    "                input_ids: torch.LongTensor,\n",
    "                scores: torch.FloatTensor,\n",
    "                **kwargs: Any,\n",
    "            ) -> bool:\n",
    "                for stop_id in stopping_ids_list:\n",
    "                    if input_ids[0][-1] == stop_id:\n",
    "                        return True\n",
    "                return False\n",
    "\n",
    "        stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "        inputs = self.pipeline.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        streamer = TextIteratorStreamer(\n",
    "            self.pipeline.tokenizer,\n",
    "            timeout=60.0,\n",
    "            skip_prompt=skip_prompt,\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        generation_kwargs = dict(\n",
    "            inputs,\n",
    "            streamer=streamer,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            **pipeline_kwargs,\n",
    "        )\n",
    "        t1 = Thread(target=self.pipeline.model.generate, kwargs=generation_kwargs)\n",
    "        t1.start()\n",
    "\n",
    "        for char in streamer:\n",
    "            chunk = GenerationChunk(text=char)\n",
    "            if run_manager:\n",
    "                run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n",
    "\n",
    "            yield chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id=\"google/gemma-2-2b-it\"\n",
    "\n",
    "pipeline_kwargs={\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": .5,\n",
    "    \"top_p\": 0.7,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "gen = pipeline(\n",
    "    task='text-generation',\n",
    "    model=gemma_2_model,\n",
    "    tokenizer=gemma_2_tokenizer,\n",
    "    # streamer=streamer,\n",
    "    device=device,\n",
    "    **pipeline_kwargs\n",
    "    )\n",
    "llm = HuggingFacePipeline(pipeline=gen, device=device, pipeline_kwargs=pipeline_kwargs)\n",
    "# llm = HuggingFacePipeline.from_model_id(model_id=model_id,\n",
    "#                                         task=\"text-generation\",\n",
    "#                                         device=0 if torch.cuda.is_available() else -1,\n",
    "#                                         model_kwargs={\n",
    "#                                             \"max_length\": 2048,\n",
    "#                                             \"temperature\": .5,\n",
    "#                                             \"top_p\": 0.7,\n",
    "#                                             \"repetition_penalty\": 1.1,\n",
    "#                                             \"do_sample\": True,\n",
    "#                                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, template=\"<bos><start_of_turn>user\\nInstructions:\\n- If the question involves a health-related issue, suggest possible causes and basic steps the user can take for relief, if applicable.\\n- You should explain in as much detail as possible what you know from the bottom of your heart to the user's questions.\\n- You can refer to the contents of the documents to create a response.\\n- Only use information that is directly related to the question.\\n- If no information is found in the documents, provide an answer based on general knowledge without fabricating details.\\n- You MUST answer in Korean.\\n\\n\\nDocuments: {documents}\\n\\nQuestion: {question}<end_of_turn>\\n<start_of_turn>model\\n\")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Instructions:\n",
    "- If the question involves a health-related issue, suggest possible causes and basic steps the user can take for relief, if applicable.\n",
    "- You should explain in as much detail as possible what you know from the bottom of your heart to the user's questions.\n",
    "- You can refer to the contents of the documents to create a response.\n",
    "- Only use information that is directly related to the question.\n",
    "- If no information is found in the documents, provide an answer based on general knowledge without fabricating details.\n",
    "- You MUST answer in Korean.\n",
    "\n",
    "\n",
    "Documents: {documents}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prev_chat = []\n",
    "\n",
    "chat = [\n",
    "    *prev_chat,\n",
    "     { \"role\": \"user\", \"content\": template}\n",
    "]\n",
    "\n",
    "prompt_template = gemma_2_tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"question\", \"documents\"], template=prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<bos><start_of_turn>user\\nInstructions:\\n- If the question involves a health-related issue, suggest possible causes and basic steps the user can take for relief, if applicable.\\n- You should explain in as much detail as possible what you know from the bottom of your heart to the user's questions.\\n- You can refer to the contents of the documents to create a response.\\n- Only use information that is directly related to the question.\\n- If no information is found in the documents, provide an answer based on general knowledge without fabricating details.\\n- You MUST answer in Korean.\\n\\n\\nDocuments: ['월경전 증후군은 월경 주기 전에 나타나는 다양한 신체적, 정신적 증상을 말합니다. 이는 여성들의 일상적인 생활에 영향을 주며, 월경전 불쾌한 증상으로 나타나기도 합니다.\\\\n월경전 증후군의 증상 중 하나는 배 아픔입니다. 일부 여성들은 배가 자주 아프고 불편함을 느낄 수 있습니다. 이러한 증상은 월경 전 몇 일에서 2주 전부터 시작되며 월경이 시작되면서 사라집니다. 복부 팽만감, 복통, 불안, 짜증, 우울 등이 월경전 증후군의 증상 중 일부일 수 있습니다.\\\\n월경전 증후군은 일시적으로 나타날 수도 있고 오랫동안 지속될 수도 있습니다. 월경전 증후군을 완화하기 위해 적절한 휴식과 충분한 수면을 취하는 것이 중요합니다. 일상 생활에서 신체적인 피로를 줄이고 정신적인 안정감을 유지하는 것도 도움이 됩니다. 그러나 증상이 지속되거나 심해지면 의사와 상담하여 적절한 치료 방법을 결정해야 합니다.', '배뇨 장애는 다양한 증상을 보일 수 있습니다.\\\\n빈뇨, 야간뇨, 요주저, 세뇨, 단절성배뇨, 요점적, 복부 힘주기, 요절박이 주요한 증상입니다. 빈뇨는 하루 평균 배뇨 횟수가 8회 이상이며 야간뇨는 잠을 자다가 화장실을 2회 이상 가야 할 정도로 자주 소변을 보는 증상입니다. 요주저는 소변줄기가 약해지고 약해지는 증상입니다. 세뇨는 소변줄기가 약해지고 약해지며, 요점적은 배뇨 후 소변이 방울방울로 떨어지는 증상입니다. 복부 힘주기는 배뇨를 하기 위해 복부에 힘을 줘야 하는 증상입니다. 마지막으로, 요절박은 소변을 참을 수 없는 경우에 소변이 나오는 것을 말합니다.\\\\n배뇨 장애는 다양한 증상을 동반할 수 있으며, 이는 비뇨기, 신장, 신경계, 근골격계의 문제로 인해 발생할 수 있습니다. 따라서 증상이 있을 경우 전문의의 진료를 받아 원인을 확인하고 치료받는 것이 중요합니다.', '배뇨장애는 다양한 증상을 나타낼 수 있습니다.\\\\n빈뇨는 하루 평균 배뇨 횟수가 8회 이상인 경우를 의미합니다. 야간뇨는 잠을 자다가 화장실을 가는 횟수가 2회 이상이며, 요주저는 일정시간이 걸리고 배뇨가 시작되는 증상입니다. 요점적은 소변줄기가 약해지는 상태를 말하며, 단절성배뇨는 배뇨 시 소변이 중간중간 끊어지는 증상입니다. 세뇨는 소변줄기가 약해지는 증상이며, 요절박은 소변을 참지 못하고 즉시 화장실에 가야 할 것 같은 느낌이 드는 증상입니다. 복부 힘주기는 배뇨하기 위해 복부에 힘을 주어야만 배뇨가 가능한 증상입니다.\\\\n배뇨장애는 다양한 증상을 나타내며, 원인에 따라 치료 방법이 다르기 때문에 의사의 지도와 조언을 따르는 것이 중요합니다.']\\n\\nQuestion: 갑자기 배가 너무 아파<end_of_turn>\\n<start_of_turn>model\\n배가 갑자기 아프다면 월경전 증후군이나 배뇨 장애 등 여러 가지 질환이 원인될 수 있습니다. 월경전 증후군에서는 배가 자주 아프고 불편함을 느낄 수 있습니다. 월경전 증후군은 일시적으로 나타날 수도 있고 오랫동안 지속될 수도 있습니다.  \\n\\n**월경전 증후군의 경우:**\\n\\n* **월경 전 몇 일에서 2주 전부터 시작되며 월경이 시작되면서 사라집니다.**\\n* **복부 팽만감, 복통, 불안, 짜증, 우울 등이 월경전 증후군의 증상 중 일부일 수 있습니다.**\\n\\n**배뇨 장애의 경우:**\\n\\n* **빈뇨, 야간뇨, 요주저, 세뇨, 단절성배뇨, 요점적, 복부 힘주기, 요절박** 등 다양한 증상이 나타날 수 있습니다.\\n\\n**하지만, 위 내용은 일반적인 정보이며, 개별적인 상황에 대한 진단 및 치료는 의사에게 문의하는 것이 가장 중요합니다.** \\n\\n**추천:**\\n\\n1. **직접 의사와 상담**: 배가 아픈 원인을 파악하고 적절한 치료법을 받으세요.\\n2. **휴식과 충분한 수면**: 스트레스를 줄이고 건강한 생활 습관을 유지하세요.\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = rag_chain.invoke(\"갑자기 배가 너무 아파\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_response(response, return_output=False):\n",
    "    answer = \"\"\n",
    "    for token in response:\n",
    "        answer += token\n",
    "        print(token, end=\"\", flush=True)\n",
    "    if return_output:\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배가 갑자기 아픈 것은 여러 가지 이유가 있을 수 있습니다. 월경전 증후군이나 배뇨 장애 등이 원인일 수도 있습니다. \n",
      "\n",
      "**월경전 증후군:** 월경전에 배가 아프는 증상이 나타날 수 있습니다. 월경 전 몇 일에서 2주 전부터 시작되며 월경이 시작되면서 사라집니다. \n",
      "\n",
      "**배뇨 장애:** 배뇨 장애는 다양한 증상을 보일 수 있습니다. 빈뇨, 야간뇨, 요주저, 세뇨, 단절성배뇨, 요점적, 복부 힘주기, 요절박이 주요한 증상입니다.  \n",
      "\n",
      "하지만, **구체적인 증상과 함께 설명해주세요.**  예를 들어, 갑작스러운 통증의 위치, 강도, 그리고 어떤 상황에서 발생하는지 알려주시면 더 정확한 정보를 제공해드릴 수 있습니다. \n",
      "\n",
      "혹시 다른 질병으로 생각하시는 건가요? \n",
      "\n",
      "* **특별히 위험한 상황**이 있는지, 혹은 **다른 증상**이 함께 나타나 있는지 알려주시면 더욱 도움이 될 것입니다. \n",
      "\n",
      "건강 문제는 매우 중요하므로, **직접적인 의학적 조언을 구하기 위해서는 의사에게 문의하는 것이 가장 좋습니다.** \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.stream(\"갑자기 배가 너무 아파\")\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
